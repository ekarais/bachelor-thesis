{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from data_utils import RegeneratedPGM as PGM\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "#from torch import logical_xor#, logical_or, logical_and\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "import warnings\n",
    "\n",
    "import sys, getopt, os\n",
    "from vae import Encoder, Decoder, VAE\n",
    "from autoencoder import Autoencoder\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_rules(x, x_rules):\n",
    "        \n",
    "        x.to(device)\n",
    "        x_rules.to(device)\n",
    "        \n",
    "        #first, convert logits to hard assignments.\n",
    "        y = x.view(-1, 3, 3, 336).float()\n",
    "        y /= 0.5\n",
    "        y += 0.1\n",
    "        sizes_oh = y[:,:,:,:99].view(-1, 3, 3, 9, 11) \n",
    "        cols_oh = y[:,:,:,99:198].view(-1, 3, 3, 9, 11)\n",
    "        types_oh = y[:,:,:,198:270].view(-1, 3, 3, 9, 8)\n",
    "        lcols_oh = y[:,:,:,270:336].view(-1, 3, 3, 6, 11)\n",
    "        \n",
    "        sizes_oh = F.one_hot(sizes_oh.argmax(4), num_classes=11)\n",
    "        cols_oh = F.one_hot(cols_oh.argmax(4), num_classes=11)\n",
    "        types_oh = F.one_hot(types_oh.argmax(4), num_classes=8)\n",
    "        lcols_oh = F.one_hot(lcols_oh.argmax(4), num_classes=11)\n",
    "        \n",
    "        #then, compute rules present in the hard assignments\n",
    "        #compute attribute sets in one-hot encoding: result should have shape (64,3,3,4,10)\n",
    "        attr_sets = torch.Tensor(64,3,3,6,10)\n",
    "        panel_positions = torch.Tensor(64,3,3,2,10)\n",
    "        \n",
    "        panel_positions[:,:,:,0] = torch.cat((torch.argmax(sizes_oh, dim=4), torch.zeros((64,3,3,1), dtype=torch.long)), axis=3) #(64,3,3,10)\n",
    "        panel_positions[:,:,:,1] = torch.cat((torch.argmax(lcols_oh, dim=4), torch.zeros((64,3,3,4), dtype=torch.long)), axis=3) #(64,3,3,10)\n",
    "        \n",
    "        attr_sets[:,:,:,0] = torch.sum(sizes_oh, dim=3)[:,:,:,1:]\n",
    "        attr_sets[:,:,:,1] = torch.sum(cols_oh, dim=3)[:,:,:,1:]\n",
    "        attr_sets[:,:,:,2] = torch.cat((torch.sum(types_oh, dim=3)[:,:,:,1:], torch.zeros((64,3,3,3), dtype=torch.long)), axis=3)\n",
    "        attr_sets[:,:,:,3] = torch.sum(lcols_oh, dim=3)[:,:,:,1:]\n",
    "        attr_sets[:,:,:,4:] = panel_positions\n",
    "        \n",
    "        attr_sets[attr_sets.nonzero(as_tuple=True)] = 1 #set all non-zero elements to 1.\n",
    "        attr_sets = attr_sets.long()\n",
    "        \n",
    "        #compute rules matrix: result should have shape (64,29)\n",
    "        rules = torch.Tensor(64,29)\n",
    "        \n",
    "        #XOR, OR, AND\n",
    "        cardinalities = torch.sum(attr_sets, dim=4) #(64,3,3,6)\n",
    "        card_flags = torch.prod(torch.prod(cardinalities, dim=1), dim=1) #(64,6) #True if set is non-empty\n",
    "        third_sets = attr_sets[:,:,2] #(64,3,6,10)\n",
    "        xors = torch.logical_xor(attr_sets[:,:,0].bool(), attr_sets[:,:,1].bool()).long() #(64,3,6,10)\n",
    "        ors = torch.logical_or(attr_sets[:,:,0].bool(), attr_sets[:,:,1].bool()).long() #(64,3,6,10)\n",
    "        ands = torch.logical_and(attr_sets[:,:,0].bool(), attr_sets[:,:,1].bool()).long() #(64,3,6,10)\n",
    "        \n",
    "        #check if constant\n",
    "        full_ors = torch.logical_or(attr_sets[:,:,2].bool(), ors.bool()).long() #(64,3,6,10)\n",
    "        full_ors_card_flags_1 = torch.prod(torch.eq(torch.sum(full_ors, dim=3), 1).long(), dim=1) #(64,6) checks if the full_ors have 1 element => implies constant rule\n",
    "        card_flags_1 = torch.eq(cardinalities, 1).long() #(64,3,3,6)\n",
    "        card_flags_1 = torch.prod(torch.prod(card_flags_1, dim=2), dim=1) #(64,6) #if all sets have 1 element\n",
    "        not_constant = 1 - torch.mul(card_flags_1, full_ors_card_flags_1) #(64,6) ==1 if the value sets are not constant\n",
    "        eq_aux_1 = torch.prod(torch.prod(torch.eq(attr_sets[:,:,0,4:], attr_sets[:,:,1,4:]).long(), dim=3), dim=1) #(64,2)\n",
    "        eq_aux_2 = torch.prod(torch.prod(torch.eq(attr_sets[:,:,1,4:], attr_sets[:,:,2,4:]).long(), dim=3), dim=1) #(64,2)\n",
    "        not_constant[:,4:] = 1 - torch.mul(eq_aux_1, eq_aux_2) #constraint slightly different for position rules\n",
    "        \n",
    "        ##can be done with one eq call, one mul call, and 2 prod calls in total, optimize!\n",
    "        raw_xor_results = torch.eq(third_sets, xors).long() #(64,3,6,10)\n",
    "        raw_or_results = torch.eq(third_sets, ors).long() #(64,3,6,10)\n",
    "        raw_and_results = torch.eq(third_sets, ands).long() #(64,3,6,10)\n",
    "        xor_results = torch.prod(torch.prod(raw_xor_results, dim=3), dim=1) #(64,6)\n",
    "        or_results = torch.prod(torch.prod(raw_or_results, dim=3), dim=1) #(64,6)\n",
    "        and_results = torch.prod(torch.prod(raw_and_results, dim=3), dim=1) #(64,6)\n",
    "        xor_r = torch.mul(torch.mul(card_flags, xor_results).bool().long(), not_constant) #(64,6)\n",
    "        or_r = torch.mul(torch.mul(card_flags, or_results).bool().long(), not_constant) #(64,6)\n",
    "        and_r = torch.mul(torch.mul(card_flags, and_results).bool().long(), not_constant) #(64,6)\n",
    "        \n",
    "        #PROGR, UNION\n",
    "        full_ors_card_flags = torch.prod(torch.eq(torch.sum(full_ors, dim=3), 3).long(), dim=1) #(64,6) checks if the full_ors have 3 elements\n",
    "        identical_order_2 = torch.logical_and(torch.eq(attr_sets[:,0], attr_sets[:,1]), torch.eq(attr_sets[:,1], attr_sets[:,2])).long() #(64,3,6,10)\n",
    "        non_identical_order_2 = 1 - torch.prod(torch.prod(identical_order_2, dim=3), dim=1) #(64,6)\n",
    "        union_flag = torch.mul(torch.eq(full_ors[:,0], full_ors[:,1]).long(), torch.eq(full_ors[:,1], full_ors[:,2]).long()) #(64,6,10) checks if the 3 unions are consistent\n",
    "        union_flag = torch.prod(union_flag, dim=2) #(64,6)\n",
    "        union_flag *= non_identical_order_2\n",
    "        progr_flag = 1 - union_flag # if the unions are inconsistent, card_flags * full_ors_card_flags * (NOT union_flag) == 1 implies progression.\n",
    "                                    # since we only have non-distractive RPMs, it MUST be the progression relation.\n",
    "        aux = torch.mul(card_flags_1, full_ors_card_flags) #intermediate product to save computation\n",
    "        union_r = torch.mul(aux, union_flag) #(64,6)\n",
    "        progr_r = torch.mul(aux, progr_flag) #(64,6)\n",
    "        \n",
    "        #NUMBER PROGR, NUMBER UNION\n",
    "        num_sets = cardinalities[:,:,:,4] #(64,3,3)\n",
    "        num_avg = torch.sum(num_sets,dim=(1,2))/9\n",
    "        non_const = 1 - torch.prod(torch.prod(torch.eq(num_sets - num_avg.view(-1,1,1), 0).long(), dim=2), dim=1)\n",
    "        #nonzero = torch.prod(torch.prod(num_sets, dim=2), dim=1).bool().long()\n",
    "        non_identical_order = 1 - torch.prod(torch.logical_and(torch.eq(num_sets[:,0], num_sets[:,1]), torch.eq(num_sets[:,1], num_sets[:,2])).long(), dim=1) #(64,3)\n",
    "        nprog_r = torch.prod(torch.logical_and(torch.lt(num_sets[:,:,0], num_sets[:,:,1]), torch.lt(num_sets[:,:,1], num_sets[:,:,2])).long(), dim=1)\n",
    "        nprog_r = torch.mul(nprog_r, non_const)\n",
    "        nunion_r = torch.logical_and(torch.eq(torch.sort(num_sets[:,0])[0], torch.sort(num_sets[:,1])[0]), torch.eq(torch.sort(num_sets[:,1])[0], torch.sort(num_sets[:,2])[0])) #(64,3)\n",
    "        nunion_r = torch.mul(torch.mul(torch.prod(nunion_r.long(), dim=1), non_const), non_identical_order)\n",
    "        #i += 1\n",
    "       \n",
    "        #Reorder computed rules to allow direct comparison (use the rules Tensor)\n",
    "        rules[:,:6] = xor_r\n",
    "        rules[:,6:12] = or_r\n",
    "        rules[:,12:18] = and_r\n",
    "        rules[:,18] = nprog_r\n",
    "        rules[:,19] = nunion_r\n",
    "        rules[:,20:24] = progr_r[:,:4]\n",
    "        rules[:,24:28]= union_r[:,:4]\n",
    "        rules[:,28] = union_r[:,5]\n",
    "        rule_mapping = np.array([20,0,6,12,24,21,1,7,13,25,18,19,4,10,16,22,2,8,14,26,23,3,9,15,27,5,11,17,28])\n",
    "        rules = rules[:,rule_mapping].long()\n",
    "        \n",
    "        position_rule_flag = torch.sum(rules[:,12:15], dim=1).long() #(64), 1 iff a shape position rule exists\n",
    "        rules[:,10:12] *= (1 - position_rule_flag).view(-1,1) #number rules can't exist if a position rule exists\n",
    "        \n",
    "        '''\n",
    "        print(rules[16,4])\n",
    "        print(x_rules[16,4])\n",
    "        print(attr_sets[16,:,:,0])\n",
    "        \n",
    "        print(torch.nonzero(1-torch.eq(rules, x_rules).long()))\n",
    "        '''\n",
    "        \n",
    "        Z = torch.sum(x_rules, dim=1)\n",
    "        Z_ = 29 - Z\n",
    "        tps = torch.sum(torch.mul(rules, x_rules), dim=1) #True Positives\n",
    "        tns = torch.sum(torch.mul(1-rules, 1-x_rules), dim=1) #True Positives\n",
    "        mean_sensitivity = torch.mean(torch.true_divide(tps, Z)) #Mean sensitivity\n",
    "        mean_specificity = torch.mean(torch.true_divide(tns, Z_))\n",
    "        \n",
    "        #print(str(i) + \" \" +str(torch.equal(rules, x_rules)))\n",
    "        return mean_sensitivity, mean_specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized dataset with  48000  samples.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data_directory = \"/home/ege/Documents/bthesis/data/onehot/neutral_48000/\"\n",
    "data = PGM(data_directory)\n",
    "iterator = DataLoader(data, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1587428266983/work/aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 True\n",
      "1 True\n",
      "2 True\n",
      "3 True\n",
      "4 True\n",
      "5 True\n",
      "6 True\n",
      "7 True\n",
      "8 True\n",
      "9 True\n",
      "10 True\n",
      "11 True\n",
      "12 True\n",
      "13 True\n",
      "14 True\n",
      "15 True\n",
      "16 True\n",
      "17 True\n",
      "18 True\n",
      "19 True\n",
      "20 True\n",
      "21 True\n",
      "22 True\n",
      "23 True\n",
      "24 True\n",
      "25 True\n",
      "26 True\n",
      "27 True\n",
      "28 True\n",
      "29 True\n",
      "30 True\n",
      "31 True\n",
      "32 True\n",
      "33 True\n",
      "34 True\n",
      "35 True\n",
      "36 True\n",
      "37 True\n",
      "38 True\n",
      "39 True\n",
      "40 True\n",
      "41 True\n",
      "42 True\n",
      "43 True\n",
      "44 True\n",
      "45 True\n",
      "46 True\n",
      "47 True\n",
      "48 True\n",
      "49 True\n",
      "50 True\n",
      "51 True\n",
      "52 True\n",
      "53 True\n",
      "54 True\n",
      "55 True\n",
      "56 True\n",
      "57 True\n",
      "58 True\n",
      "59 True\n",
      "60 True\n",
      "61 True\n",
      "62 True\n",
      "63 True\n",
      "64 True\n",
      "65 True\n",
      "66 True\n",
      "67 True\n",
      "68 True\n",
      "69 True\n",
      "70 True\n",
      "71 True\n",
      "72 True\n",
      "73 True\n",
      "74 True\n",
      "75 True\n",
      "76 True\n",
      "77 True\n",
      "78 True\n",
      "79 True\n",
      "80 True\n",
      "81 True\n",
      "82 True\n",
      "83 True\n",
      "84 True\n",
      "85 True\n",
      "86 True\n",
      "87 True\n",
      "88 True\n",
      "89 True\n",
      "90 True\n",
      "91 True\n",
      "92 True\n",
      "93 True\n",
      "94 True\n",
      "95 True\n",
      "96 True\n",
      "97 True\n",
      "98 True\n",
      "99 True\n",
      "100 True\n",
      "101 True\n",
      "102 True\n",
      "103 True\n",
      "104 True\n",
      "105 True\n",
      "106 True\n",
      "107 True\n",
      "108 True\n",
      "109 True\n",
      "110 True\n",
      "111 True\n",
      "112 True\n",
      "113 True\n",
      "114 True\n",
      "115 True\n",
      "116 True\n",
      "117 True\n",
      "118 True\n",
      "119 True\n",
      "120 True\n",
      "121 True\n",
      "122 True\n",
      "123 True\n",
      "124 True\n",
      "125 True\n",
      "126 True\n",
      "127 True\n",
      "128 True\n",
      "129 True\n",
      "130 True\n",
      "131 True\n",
      "132 True\n",
      "133 True\n",
      "134 True\n",
      "135 True\n",
      "136 True\n",
      "137 True\n",
      "138 True\n",
      "139 True\n",
      "140 True\n",
      "141 True\n",
      "142 True\n",
      "143 True\n",
      "144 True\n",
      "145 True\n",
      "146 True\n",
      "147 True\n",
      "148 True\n",
      "149 True\n",
      "150 True\n",
      "151 True\n",
      "152 True\n",
      "153 True\n",
      "154 True\n",
      "155 True\n",
      "156 True\n",
      "157 True\n",
      "158 True\n",
      "159 True\n",
      "160 True\n",
      "161 True\n",
      "162 True\n",
      "163 True\n",
      "164 True\n",
      "165 True\n",
      "166 True\n",
      "167 True\n",
      "168 True\n",
      "169 True\n",
      "170 True\n",
      "171 True\n",
      "172 True\n",
      "173 True\n",
      "174 True\n",
      "175 True\n",
      "176 True\n",
      "177 True\n",
      "178 True\n",
      "179 True\n",
      "180 True\n",
      "181 True\n",
      "182 True\n",
      "183 True\n",
      "184 True\n",
      "185 True\n",
      "186 True\n",
      "187 True\n",
      "188 True\n",
      "189 True\n",
      "190 True\n",
      "191 True\n",
      "192 True\n",
      "193 True\n",
      "194 True\n",
      "195 True\n",
      "196 True\n",
      "197 True\n",
      "198 True\n",
      "199 True\n",
      "200 True\n",
      "201 True\n",
      "202 True\n",
      "203 True\n",
      "204 True\n",
      "205 True\n",
      "206 True\n",
      "207 True\n",
      "208 True\n",
      "209 True\n",
      "210 True\n",
      "211 True\n",
      "212 True\n",
      "213 True\n",
      "214 True\n",
      "215 True\n",
      "216 True\n",
      "217 True\n",
      "218 True\n",
      "219 True\n",
      "220 True\n",
      "221 True\n",
      "222 True\n",
      "223 True\n",
      "224 True\n",
      "225 True\n",
      "226 True\n",
      "227 True\n",
      "228 True\n",
      "229 True\n",
      "230 True\n",
      "231 True\n",
      "232 True\n",
      "233 True\n",
      "234 True\n",
      "235 True\n",
      "236 True\n",
      "237 True\n",
      "238 True\n",
      "239 True\n",
      "240 True\n",
      "241 True\n",
      "242 True\n",
      "243 True\n",
      "244 True\n",
      "245 True\n",
      "246 True\n",
      "247 True\n",
      "248 True\n",
      "249 True\n",
      "250 True\n",
      "251 True\n",
      "252 True\n",
      "253 True\n",
      "254 True\n",
      "255 True\n",
      "256 True\n",
      "257 True\n",
      "258 True\n",
      "259 True\n",
      "260 True\n",
      "261 True\n",
      "262 True\n",
      "263 True\n",
      "264 True\n",
      "265 True\n",
      "266 True\n",
      "267 True\n",
      "268 True\n",
      "269 True\n",
      "270 True\n",
      "271 True\n",
      "272 True\n",
      "273 True\n",
      "274 True\n",
      "275 True\n",
      "276 True\n",
      "277 True\n",
      "278 True\n",
      "279 True\n",
      "280 True\n",
      "281 True\n",
      "282 True\n",
      "283 True\n",
      "284 True\n",
      "285 True\n",
      "286 True\n",
      "287 True\n",
      "288 True\n",
      "289 True\n",
      "290 True\n",
      "291 True\n",
      "292 True\n",
      "293 True\n",
      "294 True\n",
      "295 True\n",
      "296 True\n",
      "297 True\n",
      "298 True\n",
      "299 True\n",
      "300 True\n",
      "301 True\n",
      "302 True\n",
      "303 True\n",
      "304 True\n",
      "305 True\n",
      "306 True\n",
      "307 True\n",
      "308 True\n",
      "309 True\n",
      "310 True\n",
      "311 True\n",
      "312 True\n",
      "313 True\n",
      "314 True\n",
      "315 True\n",
      "316 True\n",
      "317 True\n",
      "318 True\n",
      "319 True\n",
      "320 True\n",
      "321 True\n",
      "322 True\n",
      "323 True\n",
      "324 True\n",
      "325 True\n",
      "326 True\n",
      "327 True\n",
      "328 True\n",
      "329 True\n",
      "330 True\n",
      "331 True\n",
      "332 True\n",
      "333 True\n",
      "334 True\n",
      "335 True\n",
      "336 True\n",
      "337 True\n",
      "338 True\n",
      "339 True\n",
      "340 True\n",
      "341 True\n",
      "342 True\n",
      "343 True\n",
      "344 True\n",
      "345 True\n",
      "346 True\n",
      "347 True\n",
      "348 True\n",
      "349 True\n",
      "350 True\n",
      "351 True\n",
      "352 True\n",
      "353 True\n",
      "354 True\n",
      "355 True\n",
      "356 True\n",
      "357 True\n",
      "358 True\n",
      "359 True\n",
      "360 True\n",
      "361 True\n",
      "362 True\n",
      "363 True\n",
      "364 True\n",
      "365 True\n",
      "366 True\n",
      "367 True\n",
      "368 True\n",
      "369 True\n",
      "370 True\n",
      "371 True\n",
      "372 True\n",
      "373 True\n",
      "374 True\n",
      "375 True\n",
      "376 True\n",
      "377 True\n",
      "378 True\n",
      "379 True\n",
      "380 True\n",
      "381 True\n",
      "382 True\n",
      "383 True\n",
      "384 True\n",
      "385 True\n",
      "386 True\n",
      "387 True\n",
      "388 True\n",
      "389 True\n",
      "390 True\n",
      "391 True\n",
      "392 True\n",
      "393 True\n",
      "394 True\n",
      "395 True\n",
      "396 True\n",
      "397 True\n",
      "398 True\n",
      "399 True\n",
      "400 True\n",
      "401 True\n",
      "402 True\n",
      "403 True\n",
      "404 True\n",
      "405 True\n",
      "406 True\n",
      "407 True\n",
      "408 True\n",
      "409 True\n",
      "410 True\n",
      "411 True\n",
      "412 True\n",
      "413 True\n",
      "414 True\n",
      "415 True\n",
      "416 True\n",
      "417 True\n",
      "418 True\n",
      "419 True\n",
      "420 True\n",
      "421 True\n",
      "422 True\n",
      "423 True\n",
      "424 True\n",
      "425 True\n",
      "426 True\n",
      "427 True\n",
      "428 True\n",
      "429 True\n",
      "430 True\n",
      "431 True\n",
      "432 True\n",
      "433 True\n",
      "434 True\n",
      "435 True\n",
      "436 True\n",
      "437 True\n",
      "438 True\n",
      "439 True\n",
      "440 True\n",
      "441 True\n",
      "442 True\n",
      "443 True\n",
      "444 True\n",
      "445 True\n",
      "446 True\n",
      "447 True\n",
      "448 True\n",
      "449 True\n",
      "450 True\n",
      "451 True\n",
      "452 True\n",
      "453 True\n",
      "454 True\n",
      "455 True\n",
      "456 True\n",
      "457 True\n",
      "458 True\n",
      "459 True\n",
      "460 True\n",
      "461 True\n",
      "462 True\n",
      "463 True\n",
      "464 True\n",
      "465 True\n",
      "466 True\n",
      "467 True\n",
      "468 True\n",
      "469 True\n",
      "470 True\n",
      "471 True\n",
      "472 True\n",
      "473 True\n",
      "474 True\n",
      "475 True\n",
      "476 True\n",
      "477 True\n",
      "478 True\n",
      "479 True\n",
      "480 True\n",
      "481 True\n",
      "482 True\n",
      "483 True\n",
      "484 True\n",
      "485 True\n",
      "486 True\n",
      "487 True\n",
      "488 True\n",
      "489 True\n",
      "490 True\n",
      "491 True\n",
      "492 True\n",
      "493 True\n",
      "494 True\n",
      "495 True\n",
      "496 True\n",
      "497 True\n",
      "498 True\n",
      "499 True\n",
      "500 True\n",
      "501 True\n",
      "502 True\n",
      "503 True\n",
      "504 True\n",
      "505 True\n",
      "506 True\n",
      "507 True\n",
      "508 True\n",
      "509 True\n",
      "510 True\n",
      "511 True\n",
      "512 True\n",
      "513 True\n",
      "514 True\n",
      "515 True\n",
      "516 True\n",
      "517 True\n",
      "518 True\n",
      "519 True\n",
      "520 True\n",
      "521 True\n",
      "522 True\n",
      "523 True\n",
      "524 True\n",
      "525 True\n",
      "526 True\n",
      "527 True\n",
      "528 True\n",
      "529 True\n",
      "530 True\n",
      "531 True\n",
      "532 True\n",
      "533 True\n",
      "534 True\n",
      "535 True\n",
      "536 True\n",
      "537 True\n",
      "538 True\n",
      "539 True\n",
      "540 True\n",
      "541 True\n",
      "542 True\n",
      "543 True\n",
      "544 True\n",
      "545 True\n",
      "546 True\n",
      "547 True\n",
      "548 True\n",
      "549 True\n",
      "550 True\n",
      "551 True\n",
      "552 True\n",
      "553 True\n",
      "554 True\n",
      "555 True\n",
      "556 True\n",
      "557 True\n",
      "558 True\n",
      "559 True\n",
      "560 True\n",
      "561 True\n",
      "562 True\n",
      "563 True\n",
      "564 True\n",
      "565 True\n",
      "566 True\n",
      "567 True\n",
      "568 True\n",
      "569 True\n",
      "570 True\n",
      "571 True\n",
      "572 True\n",
      "573 True\n",
      "574 True\n",
      "575 True\n",
      "576 True\n",
      "577 True\n",
      "578 True\n",
      "579 True\n",
      "580 True\n",
      "581 True\n",
      "582 True\n",
      "583 True\n",
      "584 True\n",
      "585 True\n",
      "586 True\n",
      "587 True\n",
      "588 True\n",
      "589 True\n",
      "590 True\n",
      "591 True\n",
      "592 True\n",
      "593 True\n",
      "594 True\n",
      "595 True\n",
      "596 True\n",
      "597 True\n",
      "598 True\n",
      "599 True\n",
      "600 True\n",
      "601 True\n",
      "602 True\n",
      "603 True\n",
      "604 True\n",
      "605 True\n",
      "606 True\n",
      "607 True\n",
      "608 True\n",
      "609 True\n",
      "610 True\n",
      "611 True\n",
      "612 True\n",
      "613 True\n",
      "614 True\n",
      "615 True\n",
      "616 True\n",
      "617 True\n",
      "618 True\n",
      "619 True\n",
      "620 True\n",
      "621 True\n",
      "622 True\n",
      "623 True\n",
      "624 True\n",
      "625 True\n",
      "626 True\n",
      "627 True\n",
      "628 True\n",
      "629 True\n",
      "630 True\n",
      "631 True\n",
      "632 True\n",
      "633 True\n",
      "634 True\n",
      "635 True\n",
      "636 True\n",
      "637 True\n",
      "638 True\n",
      "639 True\n",
      "640 True\n",
      "641 True\n",
      "642 True\n",
      "643 True\n",
      "644 True\n",
      "645 True\n",
      "646 True\n",
      "647 True\n",
      "648 True\n",
      "649 True\n",
      "650 True\n",
      "651 True\n",
      "652 True\n",
      "653 True\n",
      "654 True\n",
      "655 True\n",
      "656 True\n",
      "657 True\n",
      "658 True\n",
      "659 True\n",
      "660 True\n",
      "661 True\n",
      "662 True\n",
      "663 True\n",
      "664 True\n",
      "665 True\n",
      "666 True\n",
      "667 True\n",
      "668 True\n",
      "669 True\n",
      "670 True\n",
      "671 True\n",
      "672 True\n",
      "673 True\n",
      "674 True\n",
      "675 True\n",
      "676 True\n",
      "677 True\n",
      "678 True\n",
      "679 True\n",
      "680 True\n",
      "681 True\n",
      "682 True\n",
      "683 True\n",
      "684 True\n",
      "685 True\n",
      "686 True\n",
      "687 True\n",
      "688 True\n",
      "689 True\n",
      "690 True\n",
      "691 True\n",
      "692 True\n",
      "693 True\n",
      "694 True\n",
      "695 True\n",
      "696 True\n",
      "697 True\n",
      "698 True\n",
      "699 True\n",
      "700 True\n",
      "701 True\n",
      "702 True\n",
      "703 True\n",
      "704 True\n",
      "705 True\n",
      "706 True\n",
      "707 True\n",
      "708 True\n",
      "709 True\n",
      "710 True\n",
      "711 True\n",
      "712 True\n",
      "713 True\n",
      "714 True\n",
      "715 True\n",
      "716 True\n",
      "717 True\n",
      "718 True\n",
      "719 True\n",
      "720 True\n",
      "721 True\n",
      "722 True\n",
      "723 True\n",
      "724 True\n",
      "725 True\n",
      "726 True\n",
      "727 True\n",
      "728 True\n",
      "729 True\n",
      "730 True\n",
      "731 True\n",
      "732 True\n",
      "733 True\n",
      "734 True\n",
      "735 True\n",
      "736 True\n",
      "737 True\n",
      "738 True\n",
      "739 True\n",
      "740 True\n",
      "741 True\n",
      "742 True\n",
      "743 True\n",
      "744 True\n",
      "745 True\n",
      "746 True\n",
      "747 True\n",
      "748 True\n",
      "749 True\n"
     ]
    }
   ],
   "source": [
    "for i, (x, x_rules) in enumerate(iterator):\n",
    "    #if i == 1:\n",
    "        x.to(device)\n",
    "        x_rules.to(device)\n",
    "        \n",
    "        #first, convert logits to hard assignments.\n",
    "        y = x.view(-1, 3, 3, 336).float()\n",
    "        y /= 0.5\n",
    "        y += 0.1\n",
    "        sizes_oh = y[:,:,:,:99].view(-1, 3, 3, 9, 11) \n",
    "        cols_oh = y[:,:,:,99:198].view(-1, 3, 3, 9, 11)\n",
    "        types_oh = y[:,:,:,198:270].view(-1, 3, 3, 9, 8)\n",
    "        lcols_oh = y[:,:,:,270:336].view(-1, 3, 3, 6, 11)\n",
    "        \n",
    "        sizes_oh = F.one_hot(sizes_oh.argmax(4), num_classes=11)\n",
    "        cols_oh = F.one_hot(cols_oh.argmax(4), num_classes=11)\n",
    "        types_oh = F.one_hot(types_oh.argmax(4), num_classes=8)\n",
    "        lcols_oh = F.one_hot(lcols_oh.argmax(4), num_classes=11)\n",
    "        \n",
    "        #then, compute rules present in the hard assignments\n",
    "        #compute attribute sets in one-hot encoding: result should have shape (64,3,3,4,10)\n",
    "        attr_sets = torch.Tensor(64,3,3,6,10)\n",
    "        panel_positions = torch.Tensor(64,3,3,2,10)\n",
    "        \n",
    "        panel_positions[:,:,:,0] = torch.cat((torch.argmax(sizes_oh, dim=4), torch.zeros((64,3,3,1), dtype=torch.long)), axis=3) #(64,3,3,10)\n",
    "        panel_positions[:,:,:,1] = torch.cat((torch.argmax(lcols_oh, dim=4), torch.zeros((64,3,3,4), dtype=torch.long)), axis=3) #(64,3,3,10)\n",
    "        \n",
    "        attr_sets[:,:,:,0] = torch.sum(sizes_oh, dim=3)[:,:,:,1:]\n",
    "        attr_sets[:,:,:,1] = torch.sum(cols_oh, dim=3)[:,:,:,1:]\n",
    "        attr_sets[:,:,:,2] = torch.cat((torch.sum(types_oh, dim=3)[:,:,:,1:], torch.zeros((64,3,3,3), dtype=torch.long)), axis=3)\n",
    "        attr_sets[:,:,:,3] = torch.sum(lcols_oh, dim=3)[:,:,:,1:]\n",
    "        attr_sets[:,:,:,4:] = panel_positions\n",
    "        \n",
    "        attr_sets[attr_sets.nonzero(as_tuple=True)] = 1 #set all non-zero elements to 1.\n",
    "        attr_sets = attr_sets.long()\n",
    "        \n",
    "        #compute rules matrix: result should have shape (64,29)\n",
    "        rules = torch.Tensor(64,29)\n",
    "        \n",
    "        #XOR, OR, AND\n",
    "        cardinalities = torch.sum(attr_sets, dim=4) #(64,3,3,6)\n",
    "        card_flags = torch.prod(torch.prod(cardinalities, dim=1), dim=1) #(64,6) #True if set is non-empty\n",
    "        third_sets = attr_sets[:,:,2] #(64,3,6,10)\n",
    "        xors = torch.logical_xor(attr_sets[:,:,0].bool(), attr_sets[:,:,1].bool()).long() #(64,3,6,10)\n",
    "        ors = torch.logical_or(attr_sets[:,:,0].bool(), attr_sets[:,:,1].bool()).long() #(64,3,6,10)\n",
    "        ands = torch.logical_and(attr_sets[:,:,0].bool(), attr_sets[:,:,1].bool()).long() #(64,3,6,10)\n",
    "        \n",
    "        #check if constant\n",
    "        full_ors = torch.logical_or(attr_sets[:,:,2].bool(), ors.bool()).long() #(64,3,6,10)\n",
    "        full_ors_card_flags_1 = torch.prod(torch.eq(torch.sum(full_ors, dim=3), 1).long(), dim=1) #(64,6) checks if the full_ors have 1 element => implies constant rule\n",
    "        card_flags_1 = torch.eq(cardinalities, 1).long() #(64,3,3,6)\n",
    "        card_flags_1 = torch.prod(torch.prod(card_flags_1, dim=2), dim=1) #(64,6) #if all sets have 1 element\n",
    "        not_constant = 1 - torch.mul(card_flags_1, full_ors_card_flags_1) #(64,6) ==1 if the value sets are not constant\n",
    "        eq_aux_1 = torch.prod(torch.prod(torch.eq(attr_sets[:,:,0,4:], attr_sets[:,:,1,4:]).long(), dim=3), dim=1) #(64,2)\n",
    "        eq_aux_2 = torch.prod(torch.prod(torch.eq(attr_sets[:,:,1,4:], attr_sets[:,:,2,4:]).long(), dim=3), dim=1) #(64,2)\n",
    "        not_constant[:,4:] = 1 - torch.mul(eq_aux_1, eq_aux_2) #constraint slightly different for position rules\n",
    "        \n",
    "        ##can be done with one eq call, one mul call, and 2 prod calls in total, optimize!\n",
    "        raw_xor_results = torch.eq(third_sets, xors).long() #(64,3,6,10)\n",
    "        raw_or_results = torch.eq(third_sets, ors).long() #(64,3,6,10)\n",
    "        raw_and_results = torch.eq(third_sets, ands).long() #(64,3,6,10)\n",
    "        xor_results = torch.prod(torch.prod(raw_xor_results, dim=3), dim=1) #(64,6)\n",
    "        or_results = torch.prod(torch.prod(raw_or_results, dim=3), dim=1) #(64,6)\n",
    "        and_results = torch.prod(torch.prod(raw_and_results, dim=3), dim=1) #(64,6)\n",
    "        xor_r = torch.mul(torch.mul(card_flags, xor_results).bool().long(), not_constant) #(64,6)\n",
    "        or_r = torch.mul(torch.mul(card_flags, or_results).bool().long(), not_constant) #(64,6)\n",
    "        and_r = torch.mul(torch.mul(card_flags, and_results).bool().long(), not_constant) #(64,6)\n",
    "        \n",
    "        #PROGR, UNION\n",
    "        full_ors_card_flags = torch.prod(torch.eq(torch.sum(full_ors, dim=3), 3).long(), dim=1) #(64,6) checks if the full_ors have 3 elements\n",
    "        identical_order_2 = torch.logical_and(torch.eq(attr_sets[:,0], attr_sets[:,1]), torch.eq(attr_sets[:,1], attr_sets[:,2])).long() #(64,3,6,10)\n",
    "        non_identical_order_2 = 1 - torch.prod(torch.prod(identical_order_2, dim=3), dim=1) #(64,6)\n",
    "        union_flag = torch.mul(torch.eq(full_ors[:,0], full_ors[:,1]).long(), torch.eq(full_ors[:,1], full_ors[:,2]).long()) #(64,6,10) checks if the 3 unions are consistent\n",
    "        union_flag = torch.prod(union_flag, dim=2) #(64,6)\n",
    "        union_flag *= non_identical_order_2\n",
    "        progr_flag = 1 - union_flag # if the unions are inconsistent, card_flags * full_ors_card_flags * (NOT union_flag) == 1 implies progression.\n",
    "                                    # since we only have non-distractive RPMs, it MUST be the progression relation.\n",
    "        aux = torch.mul(card_flags_1, full_ors_card_flags) #intermediate product to save computation\n",
    "        union_r = torch.mul(aux, union_flag) #(64,6)\n",
    "        progr_r = torch.mul(aux, progr_flag) #(64,6)\n",
    "        \n",
    "        #NUMBER PROGR, NUMBER UNION\n",
    "        num_sets = cardinalities[:,:,:,4] #(64,3,3)\n",
    "        num_avg = torch.sum(num_sets,dim=(1,2))/9\n",
    "        non_const = 1 - torch.prod(torch.prod(torch.eq(num_sets - num_avg.view(-1,1,1), 0).long(), dim=2), dim=1)\n",
    "        #nonzero = torch.prod(torch.prod(num_sets, dim=2), dim=1).bool().long()\n",
    "        non_identical_order = 1 - torch.prod(torch.logical_and(torch.eq(num_sets[:,0], num_sets[:,1]), torch.eq(num_sets[:,1], num_sets[:,2])).long(), dim=1) #(64,3)\n",
    "        nprog_r = torch.prod(torch.logical_and(torch.lt(num_sets[:,:,0], num_sets[:,:,1]), torch.lt(num_sets[:,:,1], num_sets[:,:,2])).long(), dim=1)\n",
    "        nprog_r = torch.mul(nprog_r, non_const)\n",
    "        nunion_r = torch.logical_and(torch.eq(torch.sort(num_sets[:,0])[0], torch.sort(num_sets[:,1])[0]), torch.eq(torch.sort(num_sets[:,1])[0], torch.sort(num_sets[:,2])[0])) #(64,3)\n",
    "        nunion_r = torch.mul(torch.mul(torch.prod(nunion_r.long(), dim=1), non_const), non_identical_order)\n",
    "        #i += 1\n",
    "       \n",
    "        #Reorder computed rules to allow direct comparison (use the rules Tensor)\n",
    "        rules[:,:6] = xor_r\n",
    "        rules[:,6:12] = or_r\n",
    "        rules[:,12:18] = and_r\n",
    "        rules[:,18] = nprog_r\n",
    "        rules[:,19] = nunion_r\n",
    "        rules[:,20:24] = progr_r[:,:4]\n",
    "        rules[:,24:28]= union_r[:,:4]\n",
    "        rules[:,28] = union_r[:,5]\n",
    "        rule_mapping = np.array([20,0,6,12,24,21,1,7,13,25,18,19,4,10,16,22,2,8,14,26,23,3,9,15,27,5,11,17,28])\n",
    "        rules = rules[:,rule_mapping].long()\n",
    "        \n",
    "        position_rule_flag = torch.sum(rules[:,12:15], dim=1).long() #(64), 1 iff a shape position rule exists\n",
    "        rules[:,10:12] *= (1 - position_rule_flag).view(-1,1) #number rules can't exist if a position rule exists\n",
    "        \n",
    "        '''\n",
    "        print(rules[16,4])\n",
    "        print(x_rules[16,4])\n",
    "        print(attr_sets[16,:,:,0])\n",
    "        \n",
    "        print(torch.nonzero(1-torch.eq(rules, x_rules).long()))\n",
    "        '''\n",
    "        metric_1 = torch.sum(torch.mul(rules, x_rules), dim=1) #True Positives\n",
    "        metric_2 = torch.sum(rules, dim=1) #Positives\n",
    "        \n",
    "        print(str(i) + \" \" +str(torch.equal(rules, x_rules)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
